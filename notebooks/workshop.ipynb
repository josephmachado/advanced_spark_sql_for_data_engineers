{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f59b42bc-8dc6-4a28-abe7-15a9cbba6bcd",
   "metadata": {},
   "source": [
    "## Intro & CodeSpaces Setup\n",
    "\n",
    "- Workshop format: exercises & take home exercises; code along\n",
    "- setup codespaces with github\n",
    "- All answers will be sent after workshop\n",
    "- I will stay on this call for more questions \n",
    "- Videos will be transcribed and uploaded to Podia where you will have access\n",
    "- We will use TPCH data; used in warehouse benchmarking\n",
    "- restarting codespaces\n",
    "- switch off codespace to save on free time\n",
    "- you can also run locally with docker compose\n",
    "\n",
    "## Advanced SQL is making complex logic easy to understand\n",
    "\n",
    "- Advanced SQL is writing code that is understandable\n",
    "- Clear thinking/pseudo code + locality of behaviour -> easy to maintain code\n",
    "    \n",
    "### Use window functions to compare values between rows in a table ~ 1h\n",
    "\n",
    "- All transformations in SQL (except Windows) operate on one row at a time (e.g. lower(), round(), etc) or on a group of rows (GROUP BY + min/max/avg/count/sum) or combine rows from multiple tables (e.g. Joins)\n",
    "- But only windows allow you to compare values across rows\n",
    "- You can do similar thing with self join, but window functions are simpler to use and battle tested\n",
    "- If you identify logic that requires looping through a subset of the rows of a table and performing some operation that involves values from multiple rows -> Windows\n",
    "\n",
    "add: image showing difference between group by and window function\n",
    "\n",
    "#### Partition to define a set of rows to work on, Window frame to define rows in the partition to be used in the computation\n",
    "\n",
    "- Anatomy of a window function with function, partition, over, window frame and order\n",
    "- partition is optional (without it the entire table is considered a partition) & window frame is optional (without it the entire partition is considered for the function)\n",
    "- Visualize window funciton as a function that gets applied to one row at a time (defined by the order if specified)\n",
    "- When you see looping, ranking, or running aggregates think window function\n",
    "- Percentage change over months -> common pattern for which window functions are used. I was asked this in an interview many years ago and failed.\n",
    "\n",
    "Exercise, 15m, Try it now: Self join to Window function with range to define window frame\n",
    "\n",
    "complex self join and group by to get the running sum for each row (add: image) how would you convert it to a window function? \n",
    "Discuss simplicity; but with a caveat that if people don\\'t understand window function that might make this code hard to read. But most DEs are familiar with window function even if they don\\'t know how/when to use it appropriately\n",
    "\n",
    "- We can partition by multiple columns, every unique combination of values will be considered a partition (add: image)\n",
    "- You would typically only partition on columns with low cardinality (low number of unique values)\n",
    "- The order of columns in a partition does not matter, for e.g. date, state and state, date will give you the same partition\n",
    "- The order of rows specified by ORDER BY clause matters a lot!; Without order the output is non determinstic (ie it can change each time you run it); If you really don't need order by clause think about if you can get away with using group by instead of window function.\n",
    "- Without order by you'd typically be looking for some aggregate or 1-st/last-st in a partition, both of which can be done with group by (if your db has the functions)\n",
    "\n",
    "Window frame:\n",
    "\n",
    "- A window frame allows you define a set of rows within a partition over which you want to apply your function.\n",
    "- Note that the ORDER BY clause applies to the entire partition & typically window frame uses that to define the rows to be included in its computation\n",
    "- There are 2 main ways to specify a window frame: using rows and range\n",
    "- rows allow you to define the rows before and after the current row to be considered for the function\n",
    "- range allows you to define the range of values (based on order by clause) over which the function is to be run\n",
    "- Range only works with numeric and date based order by's, where you'd specify the range to consider to be included in the function computation\n",
    "\n",
    "add: image on row and range based window selection\n",
    "add: exercise 10m on how to use range\n",
    "\n",
    "**Note**: Some DB engines have more ways to define window frame, such as GROUPS in\n",
    "\n",
    "#### Aggregate other row values with aggregate functions, rank between rows with ranking functions, & access data from other rows with value functions\n",
    "\n",
    "- now that we saw when to use window functions, let\\'s dig into the specifics of the types of functions that you can use\n",
    "- There are 3 types of functions\n",
    "- Aggregate: Typically all the functions that you can use with your DBs group by (such as min/max/avg/sum/count)\n",
    "- rank: Ranking rows based on values in column(s) (ranking functions)\n",
    "- value: Functions to get another row's value. You specify which row to capture using value function such as firsta value, last value, lead/lag\n",
    "\n",
    "Exercise, 15m, Try it now: Use agg, rank & value func\n",
    "\n",
    "add: references\n",
    "\n",
    "Break ~ 10m\n",
    "\n",
    "### Avoid creating partial or duplicate data by building idempotent pipelines ~ 1h\n",
    "\n",
    "scenario: (15m)\n",
    "* you build this pipeline https://github.com/josephmachado/idempotent-data-pipeline/blob/main/parking_violation_data_pipeline.py which runs once a day\n",
    "* You realize there is an issue with upstream data, and you have to re-run it for previous 3 days\n",
    "* check the output, what do you see; use `ls -ltha` why do you hav stale data\n",
    "* how do you avoid this scenario? Without having to manually clean up output each time you have to run a backfill?\n",
    "\n",
    "#### If running your pipelines multiple times with the same input produces partial/duplicate data; it is not idempotent\n",
    "\n",
    "- idempotency may sound like a \"fancy\" FP concept, but its critical for your sanity\n",
    "- as number of piipelines increase and your workload grows you want to be able to have systems that fix themselves\n",
    "- idempotent pipelines are critical for this\n",
    "- Typically the main part that dictates if a pipeline is idempotent is the logic you use to write out data\n",
    "- the output data is the output of your pipeline\n",
    "- backfills are supported by multiple orchestartion system (e.g. Airflow, etc) but idempotency depends on how your design your pipeline\n",
    "- **NOTE** if you have multiple pipelines (with different computation logic) writing to the same table, its nearly impossible to make those pipelines idempotent \n",
    "- note to have dynamic partition (spark specific) else entire table will be ovewritten\n",
    "\n",
    "#### Date & timestamps of inputs are key attributes used to enforce output idempotency\n",
    "\n",
    "- Most tables are parititioned (we will cover this in detail later) by date or some combination of date time and other attributes\n",
    "- for most pipeline simply overwriting entire parititions will make them idempotent\n",
    "- Note that overwrite will remove all existing data and insert new data\n",
    "- There will be cases like shown below where the input loose one or more partitions the output will not be idempotent, since errant data may still remain\n",
    "add: image example where old input had day_33 and so output has day_33 paritition, but new input does not have day_33 and so the output is incorrect as day_33 will still remain\n",
    "- The key insight is that when reprocessing a dataset, you typically want to completely replace the output rather than just overwrite overlapping partitions. But this is a rare case, as input data systems often underproduce data and not over produce.\n",
    "- even if they over produce its typically duplicates so your insert overwrite strategy will handle these\n",
    "\n",
    "##### Use Insert overwrite for facts and snapshot dimensions; Use MERGE INTO for SCD2 dimensions\n",
    "\n",
    "- For fact tables the insert overwrite approach works well, since your data is typically split into days/times\n",
    "- for snapshot dimension tables (ie. entire copy created per run) insert override works as well, although for snapshot dimensions you typically need not do backfills as usualy only the latest dimensions are considered\n",
    "- for SCD2 dimensions (add: scd2 image) you want to use MERGE INTO\n",
    "- MERGE INTO allow you to combine multiple update/insert/delete into one sql query\n",
    "- Before if you had issues with one of the list of update/insert/delete statement you would have had partial data in the output\n",
    "- with merge into there are either all run or all not run \n",
    "- add: example https://www.startdataengineering.com/post/create-scd2-table-with-merge-into-with-spark-iceberg/ with images\n",
    "- add: image showing matched, not matched, not matched by source\n",
    "- Show how to build SCD2 with merge into: simple case\n",
    "\n",
    "Exercise, 15m, Try it now: Write a query that is idempotent and writes data into a SCD2 table (hint: add complexities with time range comparison)\n",
    "\n",
    "Break ~ 10m\n",
    "\n",
    "## Optimization is reducing data to process & maximizing cluster utilization\n",
    "\n",
    "### Distributed data processing sytems process data in parallel & move data between processes (aka shuffle) only when necessary ~ 15m\n",
    "\n",
    "- How distributed data is stored in storage systems\n",
    "- How distributed data processing systems, read from storage, process, shuffle (when needed) and write outputs\n",
    "- How the read -> process -> write loop is designed to stream data \n",
    "\n",
    "#### Spark has its specific jargon for things all distributed data processing systems do \n",
    "\n",
    "- How spark does this with JVM & parallelization threads\n",
    "- Spark: How applications -> jobs -> stage -> task and why spark is lazy evaluated\n",
    "\n",
    "#### Narrow tx process data in parallel & Wide tx require shuffle\n",
    "\n",
    "- characteristics of narrow tx and wide tx (shuffle)\n",
    "- Spark tries to minimize the amount of data to be shuffled as transferring data across network is expensive and can overload machine memory\n",
    "- Spark uses knowledge about the data (aka metadata) to only read data that it needs and uses AQE to figure out in-process how to minimize data transfer\n",
    "\n",
    "Exercise, 5m, Multiple choice questions: Which of the following queries will result in wide transformations?\n",
    "\n",
    "### Spark creates a plan to process data, only when we ask it to write an output ~ 30m\n",
    "\n",
    "- spark waits until it really needs to process the data (iei when we ask for it to write output) as it give Spark the ability to see everything we asked it to do and create an optimal plan\n",
    "\n",
    "#### Use EXPLAIN & Spark UI to spot bottleneck in Spark's plan and badly distributed input data\n",
    "\n",
    "- Before spark runs a process we can see what it plans to do using the query plan\n",
    "- Look for shuffles and try to minimize them \n",
    "- Check if data is filtered at the first step (aka filter/predicate pushdowns) as it reduces the total amount of data to be processed.\n",
    "- Key parts of a query plan, scan, filter, project, shuffle, join - check for broadcast if one of the table is small\n",
    "- Check SparkUI to see inflight processing\n",
    "- Spot bottlenecks via the dataframe sections and see if Spark is doing more work than necessary, Spark AQE is smart, but you know the data, logic and intention add: screenshot & example to see live\n",
    "- Use Spark UI to see if data processing is evenly distributed, is one executor overloaded while others wait? How can we re-distribute. This is usually a sign of bad input data storage/distribution add: screenshot & example to see live\n",
    "\n",
    "Exercise, 15m, Scenario: Spot the bottlenecks, let's figure out how to handle them; use python loop with spark sql -> how to optimize\n",
    "\n",
    "### Data storage pattern should depend on data's use case ~ 30m\n",
    "\n",
    "- We saw how Spark tries to only read the data it has to with filter pushdowns\n",
    "- By storing data in the right pattern we can enable Spark to read less data\n",
    "\n",
    "#### In data warehouse: data is read way more than it is processed; So optimize storage for reads\n",
    "\n",
    "- In data warehousing (which is what most data pipelines are built to handle), data is read multiple times but written to only a few times\n",
    "- given that the number of reads >> num of writes optimizing data storage for read patterns will significantly impact your warehouse performance\n",
    "\n",
    "#### Appropriate data storage pattern will reduce the amount of data to be processed\n",
    "\n",
    "- When we designate a storage pattern to a table, it is stored as part of that table's metadata\n",
    "- Spark will use this metadata to create the query plan\n",
    "- In addition to how data is stored, there is also data encoding\n",
    "- In data warehouse columnar encoding is key, as the number of columns used in analytical queries are low compared to the total number of columns most tables possess\n",
    "- While there are a few column encoding format ORC, Parquet, etc. The most popular one is Parquet\n",
    "- The distributed data is stored as individual files (ideally each 128MB, which is the right size of individual Spark tasks to process effectively) encoded as parquet\n",
    "- The parquet format is extensive,but at a high level it has 3 key sections\n",
    "- 1. footer: which indicates the column value ranges present in this file & the offset location (think of this as number saying how many lines from the top is this data at). The data in this file is split into row groups, with each row group containing chunks of column data. \n",
    "- 2. Row groups: These represents rows within the table. Each row group has all the column chunks.\n",
    "- 3. Column chunks: every column values are stored consequtively so that the reader can read chunks of data (when selecting for specific column)\n",
    "\n",
    "add: query metadata to see partition schema\n",
    "add: column format\n",
    "add: parquet: footer, rowgroup, column chunk\n",
    "\n",
    "#### Understand when to use partition/cluster/sort your data\n",
    "\n",
    "Scenario: Assume you have the files, with data being always queried by date add: query\n",
    "How will you store the data such that only the data for the respective dates are queried? \n",
    "\n",
    "add: image\n",
    "\n",
    "- partitioning is the idea of storing data as a set of folders, each folder representing a partition.\n",
    "- You can partition a table by multiple columns, but this will impact how efficient the query is.\n",
    "\n",
    "add scenario: reduce partitions scanned when partitioning by date, vs date & some state id?\n",
    "\n",
    "- Partitioning is a good fit for columns with low cardinality\n",
    "- For columns with high cardinality you can bucket them\n",
    "\n",
    "scenario: same file eg, but query is often by age\n",
    "You can bucket them into age buckets and partition them.\n",
    "\n",
    "- In spark bucketing enables you to split data based on values of columns into n distinct buckets\n",
    "- this is helpful to reduce data shuffle during joins,\n",
    "\n",
    "e.g. you bucket 2 tables by some user id and then during join time there would not be any need to shuffle data since data to be joined are already in the same bucket\n",
    "add: image\n",
    "add example: Group bys will not require shuffle \n",
    "\n",
    "- Another technique to optimize storage for high cardinality columns (columns with lot of unique values like revenue, etc) is sorting\n",
    "- with sorted data sets spark can identify the chunk of data that contains data for a range filter\n",
    "- in addition to this the join startegy sort-merge join which requires a sort on the join key can skip the sort part, if the data is sorted by the join key\n",
    "\n",
    "- typically mutliple approaches are use\n",
    "- date/time-h and some common filter column is used as partition and inside those data is sorted by the key numeric metric to support range filters\n",
    "\n",
    "Exercise, 15m, Try it now: Partition or cluster for this data and this use case\n",
    "\n",
    "Break ~ 5m\n",
    "\n",
    "## Outro, Next steps, Assignments, Slack, Feedback ~ 5m\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97a7924b-6d88-4848-acbb-e66172de33f8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
